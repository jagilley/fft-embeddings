{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "with open('/users/jasper/oai.txt', 'r') as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "# nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform train/test split\n",
    "train_docs_id = reuters.fileids(categories='trade')\n",
    "test_docs_id = reuters.fileids(categories='crude')\n",
    "\n",
    "# get train/test docs\n",
    "train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "\n",
    "# get train/test labels\n",
    "train_labels = [reuters.categories(doc_id)[0] for doc_id in train_docs_id]\n",
    "test_labels = [reuters.categories(doc_id)[0] for doc_id in test_docs_id]\n",
    "\n",
    "all_docs = train_docs + test_docs\n",
    "all_labels = train_labels + test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for train/test docs\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "train_embeddings = get_embeddings(train_docs, engine=embeddings_engine)\n",
    "test_embeddings = get_embeddings(test_docs, engine=embeddings_engine)\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/train_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(train_embeddings, f)\n",
    "with open('embeddings/test_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(test_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/train_embeddings.pkl', 'rb') as f:\n",
    "    train_embeddings = pickle.load(f)\n",
    "with open('embeddings/test_embeddings.pkl', 'rb') as f:\n",
    "    test_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.35987216\n",
      "Iteration 2, loss = 2.81982825\n",
      "Iteration 3, loss = 2.09059460\n",
      "Iteration 4, loss = 1.59987339\n",
      "Iteration 5, loss = 1.69334903\n",
      "Iteration 6, loss = 1.53592467\n",
      "Iteration 7, loss = 1.42689260\n",
      "Iteration 8, loss = 1.43523758\n",
      "Iteration 9, loss = 1.42703349\n",
      "Iteration 10, loss = 1.40735614\n",
      "Iteration 11, loss = 1.40058891\n",
      "Iteration 12, loss = 1.39625255\n",
      "Iteration 13, loss = 1.39380517\n",
      "Iteration 14, loss = 1.39057487\n",
      "Iteration 15, loss = 1.38843592\n",
      "Iteration 16, loss = 1.38688206\n",
      "Iteration 17, loss = 1.38468859\n",
      "Iteration 18, loss = 1.38221461\n",
      "Iteration 19, loss = 1.38144501\n",
      "Iteration 20, loss = 1.37866268\n",
      "Iteration 21, loss = 1.37681112\n",
      "Iteration 22, loss = 1.37417329\n",
      "Iteration 23, loss = 1.37216689\n",
      "Iteration 24, loss = 1.37007989\n",
      "Iteration 25, loss = 1.36798076\n",
      "Iteration 26, loss = 1.36531528\n",
      "Iteration 27, loss = 1.36218793\n",
      "Iteration 28, loss = 1.35850787\n",
      "Iteration 29, loss = 1.35423144\n",
      "Iteration 30, loss = 1.35206544\n",
      "Iteration 31, loss = 1.34485081\n",
      "Iteration 32, loss = 1.33905463\n",
      "Iteration 33, loss = 1.33193307\n",
      "Iteration 34, loss = 1.32391794\n",
      "Iteration 35, loss = 1.31431145\n",
      "Iteration 36, loss = 1.30400482\n",
      "Iteration 37, loss = 1.29201150\n",
      "Iteration 38, loss = 1.28003466\n",
      "Iteration 39, loss = 1.26862205\n",
      "Iteration 40, loss = 1.25257504\n",
      "Iteration 41, loss = 1.24181949\n",
      "Iteration 42, loss = 1.22543262\n",
      "Iteration 43, loss = 1.20911096\n",
      "Iteration 44, loss = 1.19608276\n",
      "Iteration 45, loss = 1.18326041\n",
      "Iteration 46, loss = 1.17731108\n",
      "Iteration 47, loss = 1.16497980\n",
      "Iteration 48, loss = 1.14771568\n",
      "Iteration 49, loss = 1.13616978\n",
      "Iteration 50, loss = 1.12283378\n",
      "Iteration 51, loss = 1.11345813\n",
      "Iteration 52, loss = 1.09803426\n",
      "Iteration 53, loss = 1.08900391\n",
      "Iteration 54, loss = 1.07680898\n",
      "Iteration 55, loss = 1.05922765\n",
      "Iteration 56, loss = 1.05109442\n",
      "Iteration 57, loss = 1.03400889\n",
      "Iteration 58, loss = 1.02172935\n",
      "Iteration 59, loss = 1.02796753\n",
      "Iteration 60, loss = 1.08245514\n",
      "Iteration 61, loss = 1.33309389\n",
      "Iteration 62, loss = 1.15557610\n",
      "Iteration 63, loss = 1.14025932\n",
      "Iteration 64, loss = 1.04242972\n",
      "Iteration 65, loss = 1.03775396\n",
      "Iteration 66, loss = 0.96263954\n",
      "Iteration 67, loss = 0.95403292\n",
      "Iteration 68, loss = 0.96533831\n",
      "Iteration 69, loss = 0.97008098\n",
      "Iteration 70, loss = 0.93043394\n",
      "Iteration 71, loss = 0.91437014\n",
      "Iteration 72, loss = 0.90147206\n",
      "Iteration 73, loss = 0.92450140\n",
      "Iteration 74, loss = 0.98346887\n",
      "Iteration 75, loss = 1.08595450\n",
      "Iteration 76, loss = 1.08170492\n",
      "Iteration 77, loss = 1.07975072\n",
      "Iteration 78, loss = 0.96606736\n",
      "Iteration 79, loss = 0.85397783\n",
      "Iteration 80, loss = 0.84104777\n",
      "Iteration 81, loss = 0.84754016\n",
      "Iteration 82, loss = 0.90350898\n",
      "Iteration 83, loss = 0.94348666\n",
      "Iteration 84, loss = 0.82788485\n",
      "Iteration 85, loss = 0.80033943\n",
      "Iteration 86, loss = 0.84497504\n",
      "Iteration 87, loss = 0.89230156\n",
      "Iteration 88, loss = 0.83569339\n",
      "Iteration 89, loss = 0.86816933\n",
      "Iteration 90, loss = 0.98469919\n",
      "Iteration 91, loss = 0.89590295\n",
      "Iteration 92, loss = 0.86134384\n",
      "Iteration 93, loss = 0.77056116\n",
      "Iteration 94, loss = 0.74080215\n",
      "Iteration 95, loss = 0.74900014\n",
      "Iteration 96, loss = 0.83217741\n",
      "Iteration 97, loss = 0.78626608\n",
      "Iteration 98, loss = 0.72442278\n",
      "Iteration 99, loss = 0.72099735\n",
      "Iteration 100, loss = 0.69279070\n",
      "Iteration 101, loss = 0.70167991\n",
      "Iteration 102, loss = 0.80088702\n",
      "Iteration 103, loss = 0.86487965\n",
      "Iteration 104, loss = 0.83316790\n",
      "Iteration 105, loss = 0.71099321\n",
      "Iteration 106, loss = 0.74665481\n",
      "Iteration 107, loss = 0.69321824\n",
      "Iteration 108, loss = 0.66084895\n",
      "Iteration 109, loss = 0.65650525\n",
      "Iteration 110, loss = 0.64611504\n",
      "Iteration 111, loss = 0.64681050\n",
      "Iteration 112, loss = 0.63478867\n",
      "Iteration 113, loss = 0.61556211\n",
      "Iteration 114, loss = 0.60374109\n",
      "Iteration 115, loss = 0.63320659\n",
      "Iteration 116, loss = 0.67563763\n",
      "Iteration 117, loss = 0.77445338\n",
      "Iteration 118, loss = 1.24227062\n",
      "Iteration 119, loss = 0.97517569\n",
      "Iteration 120, loss = 0.62931694\n",
      "Iteration 121, loss = 0.59735056\n",
      "Iteration 122, loss = 0.58453964\n",
      "Iteration 123, loss = 0.57929152\n",
      "Iteration 124, loss = 0.56555452\n",
      "Iteration 125, loss = 0.55934169\n",
      "Iteration 126, loss = 0.55247592\n",
      "Iteration 127, loss = 0.54859649\n",
      "Iteration 128, loss = 0.54594045\n",
      "Iteration 129, loss = 0.54224334\n",
      "Iteration 130, loss = 0.54494634\n",
      "Iteration 131, loss = 0.53175345\n",
      "Iteration 132, loss = 0.53876838\n",
      "Iteration 133, loss = 0.51119037\n",
      "Iteration 134, loss = 0.50394758\n",
      "Iteration 135, loss = 0.49794090\n",
      "Iteration 136, loss = 0.50013528\n",
      "Iteration 137, loss = 0.52153008\n",
      "Iteration 138, loss = 0.54150861\n",
      "Iteration 139, loss = 0.64333723\n",
      "Iteration 140, loss = 0.98265323\n",
      "Iteration 141, loss = 1.72576311\n",
      "Iteration 142, loss = 0.80244131\n",
      "Iteration 143, loss = 0.53835113\n",
      "Iteration 144, loss = 0.52885250\n",
      "Iteration 145, loss = 0.52531338\n",
      "Iteration 146, loss = 0.51432511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.7835051546391752\n"
     ]
    }
   ],
   "source": [
    "# vanilla classification\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_embeddings, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fft classification with sliding windows\n",
    "\n",
    "from split_utils import split_text\n",
    "\n",
    "all_docs_paras = [split_text(doc, segment_length=40) for doc in all_docs]\n",
    "\n",
    "# remove any empty paragraphs\n",
    "all_docs_paras = [[para for para in paras if para] for paras in all_docs_paras]\n",
    "# remove any '' paragraphs\n",
    "all_docs_paras = [[para for para in paras if para != ''] for paras in all_docs_paras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [11:05<00:00,  1.60it/s] \n"
     ]
    }
   ],
   "source": [
    "# get embeddings for each paragraph\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "train_embeddings_paras = [get_embeddings(paras, engine=embeddings_engine) for paras in tqdm(all_docs_paras)]\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'wb') as f:\n",
    "    pickle.dump(train_embeddings_paras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [00:03<00:00, 273.26it/s]\n",
      "100%|██████████| 1063/1063 [00:04<00:00, 229.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'rb') as f:\n",
    "    all_embeddings_paras = pickle.load(f)\n",
    "\n",
    "# convert to numpy arrays\n",
    "all_embeddings_paras = [np.array(doc) for doc in all_embeddings_paras]\n",
    "\n",
    "# get FFTs\n",
    "def get_fft(embedding):\n",
    "    return np.abs(librosa.stft(embedding, n_fft=32, win_length=4))\n",
    "\n",
    "# lowpass filter\n",
    "def lowpass_filter(fft, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Lowpass filter for FFTs\n",
    "    \"\"\"\n",
    "    fft = fft.copy()\n",
    "    fft[:, int(cutoff*fft.shape[1]):] = 0\n",
    "    return fft\n",
    "\n",
    "# convert back to embeddings\n",
    "def fft_to_embedding(fft):\n",
    "    return librosa.istft(fft, win_length=4)\n",
    "\n",
    "# get FFTs\n",
    "all_embeddings_paras_fft = [get_fft(embedding) for embedding in tqdm(all_embeddings_paras)]\n",
    "\n",
    "# lowpass filter\n",
    "all_embeddings_paras_fft_lowpass = [lowpass_filter(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "# convert back to embeddings\n",
    "all_embeddings_paras_lowpass = [fft_to_embedding(fft) for fft in tqdm(all_embeddings_paras_fft_lowpass)]\n",
    "\n",
    "# average embeddings\n",
    "train_embeddings_lowpass_avg = [np.mean(embeddings, axis=0) for embeddings in all_embeddings_paras_lowpass]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.77268064\n",
      "Iteration 2, loss = 1.64942340\n",
      "Iteration 3, loss = 1.51929278\n",
      "Iteration 4, loss = 1.42771212\n",
      "Iteration 5, loss = 1.39061975\n",
      "Iteration 6, loss = 1.38293971\n",
      "Iteration 7, loss = 1.38056410\n",
      "Iteration 8, loss = 1.38080339\n",
      "Iteration 9, loss = 1.37944224\n",
      "Iteration 10, loss = 1.37860835\n",
      "Iteration 11, loss = 1.37678588\n",
      "Iteration 12, loss = 1.37513812\n",
      "Iteration 13, loss = 1.37406435\n",
      "Iteration 14, loss = 1.37327178\n",
      "Iteration 15, loss = 1.37371383\n",
      "Iteration 16, loss = 1.37490806\n",
      "Iteration 17, loss = 1.37811615\n",
      "Iteration 18, loss = 1.37555660\n",
      "Iteration 19, loss = 1.37564448\n",
      "Iteration 20, loss = 1.37436271\n",
      "Iteration 21, loss = 1.37639790\n",
      "Iteration 22, loss = 1.37664942\n",
      "Iteration 23, loss = 1.37654497\n",
      "Iteration 24, loss = 1.37377315\n",
      "Iteration 25, loss = 1.37302011\n",
      "Iteration 26, loss = 1.37718225\n",
      "Iteration 27, loss = 1.37292278\n",
      "Iteration 28, loss = 1.37401421\n",
      "Iteration 29, loss = 1.37251498\n",
      "Iteration 30, loss = 1.37645613\n",
      "Iteration 31, loss = 1.37729861\n",
      "Iteration 32, loss = 1.37373496\n",
      "Iteration 33, loss = 1.37412868\n",
      "Iteration 34, loss = 1.37392832\n",
      "Iteration 35, loss = 1.37383327\n",
      "Iteration 36, loss = 1.37473761\n",
      "Iteration 37, loss = 1.37411518\n",
      "Iteration 38, loss = 1.37199399\n",
      "Iteration 39, loss = 1.37535832\n",
      "Iteration 40, loss = 1.37272216\n",
      "Iteration 41, loss = 1.37346125\n",
      "Iteration 42, loss = 1.38171804\n",
      "Iteration 43, loss = 1.37563333\n",
      "Iteration 44, loss = 1.37247556\n",
      "Iteration 45, loss = 1.37389901\n",
      "Iteration 46, loss = 1.37894200\n",
      "Iteration 47, loss = 1.37675255\n",
      "Iteration 48, loss = 1.37312200\n",
      "Iteration 49, loss = 1.37377565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.49765258215962443\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_embeddings_lowpass_avg, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e67b88d6ad197c9402ec873fcbcf9f15e38aeaf8485f1023dc62e85e716efbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
