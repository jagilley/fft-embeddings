{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk openai tqdm pandas numpy matplotlib librosa soundfile plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "with open('/users/jasper/oai.txt', 'r') as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "# nltk.download('reuters')\n",
    "\n",
    "trade_docs = reuters.fileids(categories='trade')\n",
    "crude_docs = reuters.fileids(categories='crude')\n",
    "\n",
    "all_docs = [reuters.raw(doc_id) for doc_id in trade_docs + crude_docs]\n",
    "all_labels = ['trade' for _ in trade_docs] + ['crude' for _ in crude_docs]\n",
    "\n",
    "# shuffle docs and labels together\n",
    "np.random.seed(42)\n",
    "combined = list(zip(all_docs, all_labels))\n",
    "np.random.shuffle(combined)\n",
    "all_docs, all_labels = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for train/test docs\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "all_embeddings = get_embeddings(all_docs, engine=embeddings_engine)\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'rb') as f:\n",
    "    all_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70101367\n",
      "Iteration 2, loss = 0.68367065\n",
      "Iteration 3, loss = 0.68158052\n",
      "Iteration 4, loss = 0.66672913\n",
      "Iteration 5, loss = 0.64261986\n",
      "Iteration 6, loss = 0.59763393\n",
      "Iteration 7, loss = 0.50344354\n",
      "Iteration 8, loss = 0.34982035\n",
      "Iteration 9, loss = 0.18737424\n",
      "Iteration 10, loss = 0.09429573\n",
      "Iteration 11, loss = 0.06038294\n",
      "Iteration 12, loss = 0.04866964\n",
      "Iteration 13, loss = 0.04410997\n",
      "Iteration 14, loss = 0.04180204\n",
      "Iteration 15, loss = 0.04022787\n",
      "Iteration 16, loss = 0.03893421\n",
      "Iteration 17, loss = 0.03733961\n",
      "Iteration 18, loss = 0.03657173\n",
      "Iteration 19, loss = 0.03535737\n",
      "Iteration 20, loss = 0.03205988\n",
      "Iteration 21, loss = 0.03007697\n",
      "Iteration 22, loss = 0.03237719\n",
      "Iteration 23, loss = 0.03248144\n",
      "Iteration 24, loss = 0.03481693\n",
      "Iteration 25, loss = 0.02949732\n",
      "Iteration 26, loss = 0.02617889\n",
      "Iteration 27, loss = 0.02856872\n",
      "Iteration 28, loss = 0.02560701\n",
      "Iteration 29, loss = 0.02881651\n",
      "Iteration 30, loss = 0.02308987\n",
      "Iteration 31, loss = 0.02350713\n",
      "Iteration 32, loss = 0.02478771\n",
      "Iteration 33, loss = 0.02227817\n",
      "Iteration 34, loss = 0.02207347\n",
      "Iteration 35, loss = 0.02317622\n",
      "Iteration 36, loss = 0.02158565\n",
      "Iteration 37, loss = 0.02364045\n",
      "Iteration 38, loss = 0.01880637\n",
      "Iteration 39, loss = 0.01758825\n",
      "Iteration 40, loss = 0.01852360\n",
      "Iteration 41, loss = 0.01759473\n",
      "Iteration 42, loss = 0.01783700\n",
      "Iteration 43, loss = 0.02299533\n",
      "Iteration 44, loss = 0.01727168\n",
      "Iteration 45, loss = 0.01813288\n",
      "Iteration 46, loss = 0.01666761\n",
      "Iteration 47, loss = 0.02003449\n",
      "Iteration 48, loss = 0.01504644\n",
      "Iteration 49, loss = 0.01478703\n",
      "Iteration 50, loss = 0.01576593\n",
      "Iteration 51, loss = 0.01540480\n",
      "Iteration 52, loss = 0.01541519\n",
      "Iteration 53, loss = 0.01465030\n",
      "Iteration 54, loss = 0.01377156\n",
      "Iteration 55, loss = 0.01424767\n",
      "Iteration 56, loss = 0.01746373\n",
      "Iteration 57, loss = 0.01439430\n",
      "Iteration 58, loss = 0.01319229\n",
      "Iteration 59, loss = 0.01402690\n",
      "Iteration 60, loss = 0.01424382\n",
      "Iteration 61, loss = 0.02091159\n",
      "Iteration 62, loss = 0.01659727\n",
      "Iteration 63, loss = 0.01669524\n",
      "Iteration 64, loss = 0.01386090\n",
      "Iteration 65, loss = 0.01294405\n",
      "Iteration 66, loss = 0.01263194\n",
      "Iteration 67, loss = 0.02724138\n",
      "Iteration 68, loss = 0.01303231\n",
      "Iteration 69, loss = 0.01292196\n",
      "Iteration 70, loss = 0.01219772\n",
      "Iteration 71, loss = 0.01202679\n",
      "Iteration 72, loss = 0.01183018\n",
      "Iteration 73, loss = 0.01151628\n",
      "Iteration 74, loss = 0.01149055\n",
      "Iteration 75, loss = 0.01491615\n",
      "Iteration 76, loss = 0.01155718\n",
      "Iteration 77, loss = 0.01093403\n",
      "Iteration 78, loss = 0.01394764\n",
      "Iteration 79, loss = 0.01189051\n",
      "Iteration 80, loss = 0.01210025\n",
      "Iteration 81, loss = 0.01237976\n",
      "Iteration 82, loss = 0.01370664\n",
      "Iteration 83, loss = 0.01246402\n",
      "Iteration 84, loss = 0.02993329\n",
      "Iteration 85, loss = 0.01055846\n",
      "Iteration 86, loss = 0.01027584\n",
      "Iteration 87, loss = 0.01094254\n",
      "Iteration 88, loss = 0.00988806\n",
      "Iteration 89, loss = 0.01161955\n",
      "Iteration 90, loss = 0.01052196\n",
      "Iteration 91, loss = 0.01104565\n",
      "Iteration 92, loss = 0.01059746\n",
      "Iteration 93, loss = 0.01125321\n",
      "Iteration 94, loss = 0.01120117\n",
      "Iteration 95, loss = 0.01322706\n",
      "Iteration 96, loss = 0.01062074\n",
      "Iteration 97, loss = 0.00947486\n",
      "Iteration 98, loss = 0.00988814\n",
      "Iteration 99, loss = 0.01647220\n",
      "Iteration 100, loss = 0.01106361\n",
      "Iteration 101, loss = 0.01140965\n",
      "Iteration 102, loss = 0.01214959\n",
      "Iteration 103, loss = 0.00888108\n",
      "Iteration 104, loss = 0.00881171\n",
      "Iteration 105, loss = 0.01009654\n",
      "Iteration 106, loss = 0.00998920\n",
      "Iteration 107, loss = 0.01048154\n",
      "Iteration 108, loss = 0.00977419\n",
      "Iteration 109, loss = 0.00887449\n",
      "Iteration 110, loss = 0.01240393\n",
      "Iteration 111, loss = 0.01236069\n",
      "Iteration 112, loss = 0.01072246\n",
      "Iteration 113, loss = 0.00912593\n",
      "Iteration 114, loss = 0.00955052\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "# vanilla classification\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_embeddings, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fft classification with sliding windows\n",
    "\n",
    "from split_utils import split_text\n",
    "\n",
    "all_docs_paras = [split_text(doc, segment_length=40) for doc in all_docs]\n",
    "\n",
    "# remove any empty paragraphs\n",
    "all_docs_paras = [[para for para in paras if para] for paras in all_docs_paras]\n",
    "# remove any '' paragraphs\n",
    "all_docs_paras = [[para for para in paras if para != ''] for paras in all_docs_paras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [02:51<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for each paragraph\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "train_embeddings_paras = [get_embeddings(paras, engine=embeddings_engine) for paras in tqdm(all_docs_paras)]\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'wb') as f:\n",
    "    pickle.dump(train_embeddings_paras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [00:03<00:00, 294.35it/s]\n",
      "100%|██████████| 1063/1063 [00:21<00:00, 49.71it/s]\n",
      "100%|██████████| 1063/1063 [00:08<00:00, 123.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'rb') as f:\n",
    "    all_embeddings_paras = pickle.load(f)\n",
    "\n",
    "# convert to numpy arrays\n",
    "all_embeddings_paras = [np.array(doc) for doc in all_embeddings_paras]\n",
    "\n",
    "# get FFTs\n",
    "def get_fft(embedding):\n",
    "    return librosa.stft(embedding, n_fft=32, win_length=4)\n",
    "\n",
    "# lowpass filter\n",
    "def lowpass_filter(fft, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Lowpass filter for FFTs\n",
    "    \"\"\"\n",
    "    fft = fft.copy()\n",
    "    fft[:, int(cutoff*fft.shape[1]):] = 0\n",
    "    return fft\n",
    "\n",
    "# convert back to embeddings\n",
    "def fft_to_embedding(fft):\n",
    "    return librosa.istft(fft, win_length=4)\n",
    "\n",
    "apply_lowpass = True\n",
    "\n",
    "# get FFTs\n",
    "all_embeddings_paras_fft = [get_fft(embedding) for embedding in tqdm(all_embeddings_paras)]\n",
    "\n",
    "if apply_lowpass:\n",
    "    # lowpass filter\n",
    "    all_embeddings_paras_fft = [lowpass_filter(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "# convert back to embeddings\n",
    "all_embeddings_paras_lowpass = [fft_to_embedding(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "if not apply_lowpass:\n",
    "    # assert that the embeddings are the same if lowpass filtering is not applied\n",
    "    assert np.allclose(all_embeddings_paras_lowpass[0], all_embeddings_paras[0])\n",
    "\n",
    "# average embeddings\n",
    "train_embeddings_lowpass_avg = [np.mean(embeddings, axis=0) for embeddings in all_embeddings_paras_lowpass]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70160264\n",
      "Iteration 2, loss = 0.68771710\n",
      "Iteration 3, loss = 0.69109112\n",
      "Iteration 4, loss = 0.68691152\n",
      "Iteration 5, loss = 0.68390527\n",
      "Iteration 6, loss = 0.68228892\n",
      "Iteration 7, loss = 0.67890381\n",
      "Iteration 8, loss = 0.67186771\n",
      "Iteration 9, loss = 0.66549705\n",
      "Iteration 10, loss = 0.65580546\n",
      "Iteration 11, loss = 0.63752351\n",
      "Iteration 12, loss = 0.61125691\n",
      "Iteration 13, loss = 0.57271323\n",
      "Iteration 14, loss = 0.51937137\n",
      "Iteration 15, loss = 0.42569202\n",
      "Iteration 16, loss = 0.33487200\n",
      "Iteration 17, loss = 0.58026144\n",
      "Iteration 18, loss = 0.59603981\n",
      "Iteration 19, loss = 0.15659298\n",
      "Iteration 20, loss = 0.12407522\n",
      "Iteration 21, loss = 0.09675120\n",
      "Iteration 22, loss = 0.08256976\n",
      "Iteration 23, loss = 0.08335760\n",
      "Iteration 24, loss = 0.09011859\n",
      "Iteration 25, loss = 0.07379848\n",
      "Iteration 26, loss = 0.05833198\n",
      "Iteration 27, loss = 0.05842540\n",
      "Iteration 28, loss = 0.05186314\n",
      "Iteration 29, loss = 0.05113259\n",
      "Iteration 30, loss = 0.05009799\n",
      "Iteration 31, loss = 0.04764648\n",
      "Iteration 32, loss = 0.05111239\n",
      "Iteration 33, loss = 0.04446468\n",
      "Iteration 34, loss = 0.04470120\n",
      "Iteration 35, loss = 0.04307250\n",
      "Iteration 36, loss = 0.04149480\n",
      "Iteration 37, loss = 0.04629966\n",
      "Iteration 38, loss = 0.04079966\n",
      "Iteration 39, loss = 0.04121313\n",
      "Iteration 40, loss = 0.04151447\n",
      "Iteration 41, loss = 0.04344404\n",
      "Iteration 42, loss = 0.04288222\n",
      "Iteration 43, loss = 0.05959902\n",
      "Iteration 44, loss = 0.04037155\n",
      "Iteration 45, loss = 0.05368726\n",
      "Iteration 46, loss = 0.03941357\n",
      "Iteration 47, loss = 0.04512136\n",
      "Iteration 48, loss = 0.03975913\n",
      "Iteration 49, loss = 0.03806215\n",
      "Iteration 50, loss = 0.03710318\n",
      "Iteration 51, loss = 0.03726552\n",
      "Iteration 52, loss = 0.03944627\n",
      "Iteration 53, loss = 0.03702439\n",
      "Iteration 54, loss = 0.03495258\n",
      "Iteration 55, loss = 0.03630210\n",
      "Iteration 56, loss = 0.04333328\n",
      "Iteration 57, loss = 0.03546930\n",
      "Iteration 58, loss = 0.03295422\n",
      "Iteration 59, loss = 0.03658021\n",
      "Iteration 60, loss = 0.03492550\n",
      "Iteration 61, loss = 0.03796107\n",
      "Iteration 62, loss = 0.03735770\n",
      "Iteration 63, loss = 0.03728185\n",
      "Iteration 64, loss = 0.03309513\n",
      "Iteration 65, loss = 0.03209976\n",
      "Iteration 66, loss = 0.03265315\n",
      "Iteration 67, loss = 0.04860008\n",
      "Iteration 68, loss = 0.03215891\n",
      "Iteration 69, loss = 0.03946989\n",
      "Iteration 70, loss = 0.03424250\n",
      "Iteration 71, loss = 0.03573354\n",
      "Iteration 72, loss = 0.03182982\n",
      "Iteration 73, loss = 0.03831641\n",
      "Iteration 74, loss = 0.03561134\n",
      "Iteration 75, loss = 0.05246472\n",
      "Iteration 76, loss = 0.03461476\n",
      "Iteration 77, loss = 0.04126148\n",
      "Iteration 78, loss = 0.03177440\n",
      "Iteration 79, loss = 0.03127358\n",
      "Iteration 80, loss = 0.03140351\n",
      "Iteration 81, loss = 0.03024474\n",
      "Iteration 82, loss = 0.03099807\n",
      "Iteration 83, loss = 0.03262304\n",
      "Iteration 84, loss = 0.05096102\n",
      "Iteration 85, loss = 0.03034353\n",
      "Iteration 86, loss = 0.03048500\n",
      "Iteration 87, loss = 0.02836905\n",
      "Iteration 88, loss = 0.02819884\n",
      "Iteration 89, loss = 0.02746840\n",
      "Iteration 90, loss = 0.02780090\n",
      "Iteration 91, loss = 0.03442306\n",
      "Iteration 92, loss = 0.02717826\n",
      "Iteration 93, loss = 0.02671383\n",
      "Iteration 94, loss = 0.03193259\n",
      "Iteration 95, loss = 0.03395540\n",
      "Iteration 96, loss = 0.03356982\n",
      "Iteration 97, loss = 0.02875720\n",
      "Iteration 98, loss = 0.02643677\n",
      "Iteration 99, loss = 0.04831854\n",
      "Iteration 100, loss = 0.03267172\n",
      "Iteration 101, loss = 0.04061370\n",
      "Iteration 102, loss = 0.03697583\n",
      "Iteration 103, loss = 0.02742653\n",
      "Iteration 104, loss = 0.02677668\n",
      "Iteration 105, loss = 0.02837841\n",
      "Iteration 106, loss = 0.02828290\n",
      "Iteration 107, loss = 0.02354567\n",
      "Iteration 108, loss = 0.02483371\n",
      "Iteration 109, loss = 0.02437329\n",
      "Iteration 110, loss = 0.04150156\n",
      "Iteration 111, loss = 0.02503276\n",
      "Iteration 112, loss = 0.03616304\n",
      "Iteration 113, loss = 0.02803025\n",
      "Iteration 114, loss = 0.02278692\n",
      "Iteration 115, loss = 0.03038473\n",
      "Iteration 116, loss = 0.02303833\n",
      "Iteration 117, loss = 0.03043725\n",
      "Iteration 118, loss = 0.02271913\n",
      "Iteration 119, loss = 0.02560276\n",
      "Iteration 120, loss = 0.02506373\n",
      "Iteration 121, loss = 0.02475694\n",
      "Iteration 122, loss = 0.02292886\n",
      "Iteration 123, loss = 0.04104777\n",
      "Iteration 124, loss = 0.02957072\n",
      "Iteration 125, loss = 0.02611617\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9765258215962441\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_embeddings_lowpass_avg, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results\n",
    "\n",
    "- get embeddings for whole text: 97.1% accuracy\n",
    "- sliding window without lowpass filter: 96% accuracy\n",
    "- sliding window with lowpass filter: 97.6% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e67b88d6ad197c9402ec873fcbcf9f15e38aeaf8485f1023dc62e85e716efbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
