{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk openai tqdm pandas numpy matplotlib librosa soundfile plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "with open('/users/jasper/oai.txt', 'r') as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "# nltk.download('reuters')\n",
    "\n",
    "trade_docs = reuters.fileids(categories='trade')\n",
    "crude_docs = reuters.fileids(categories='crude')\n",
    "\n",
    "all_docs = [reuters.raw(doc_id) for doc_id in trade_docs + crude_docs]\n",
    "all_labels = ['trade' for _ in trade_docs] + ['crude' for _ in crude_docs]\n",
    "\n",
    "# shuffle docs and labels together\n",
    "np.random.seed(42)\n",
    "combined = list(zip(all_docs, all_labels))\n",
    "np.random.shuffle(combined)\n",
    "all_docs, all_labels = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for train/test docs\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "all_embeddings = get_embeddings(all_docs, engine=embeddings_engine)\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'rb') as f:\n",
    "    all_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70101367\n",
      "Iteration 2, loss = 0.68367065\n",
      "Iteration 3, loss = 0.68158052\n",
      "Iteration 4, loss = 0.66672913\n",
      "Iteration 5, loss = 0.64261986\n",
      "Iteration 6, loss = 0.59763393\n",
      "Iteration 7, loss = 0.50344354\n",
      "Iteration 8, loss = 0.34982035\n",
      "Iteration 9, loss = 0.18737424\n",
      "Iteration 10, loss = 0.09429573\n",
      "Iteration 11, loss = 0.06038294\n",
      "Iteration 12, loss = 0.04866964\n",
      "Iteration 13, loss = 0.04410997\n",
      "Iteration 14, loss = 0.04180204\n",
      "Iteration 15, loss = 0.04022787\n",
      "Iteration 16, loss = 0.03893421\n",
      "Iteration 17, loss = 0.03733961\n",
      "Iteration 18, loss = 0.03657173\n",
      "Iteration 19, loss = 0.03535737\n",
      "Iteration 20, loss = 0.03205988\n",
      "Iteration 21, loss = 0.03007697\n",
      "Iteration 22, loss = 0.03237719\n",
      "Iteration 23, loss = 0.03248144\n",
      "Iteration 24, loss = 0.03481693\n",
      "Iteration 25, loss = 0.02949732\n",
      "Iteration 26, loss = 0.02617889\n",
      "Iteration 27, loss = 0.02856872\n",
      "Iteration 28, loss = 0.02560701\n",
      "Iteration 29, loss = 0.02881651\n",
      "Iteration 30, loss = 0.02308987\n",
      "Iteration 31, loss = 0.02350713\n",
      "Iteration 32, loss = 0.02478771\n",
      "Iteration 33, loss = 0.02227817\n",
      "Iteration 34, loss = 0.02207347\n",
      "Iteration 35, loss = 0.02317622\n",
      "Iteration 36, loss = 0.02158565\n",
      "Iteration 37, loss = 0.02364045\n",
      "Iteration 38, loss = 0.01880637\n",
      "Iteration 39, loss = 0.01758825\n",
      "Iteration 40, loss = 0.01852360\n",
      "Iteration 41, loss = 0.01759473\n",
      "Iteration 42, loss = 0.01783700\n",
      "Iteration 43, loss = 0.02299533\n",
      "Iteration 44, loss = 0.01727168\n",
      "Iteration 45, loss = 0.01813288\n",
      "Iteration 46, loss = 0.01666761\n",
      "Iteration 47, loss = 0.02003449\n",
      "Iteration 48, loss = 0.01504644\n",
      "Iteration 49, loss = 0.01478703\n",
      "Iteration 50, loss = 0.01576593\n",
      "Iteration 51, loss = 0.01540480\n",
      "Iteration 52, loss = 0.01541519\n",
      "Iteration 53, loss = 0.01465030\n",
      "Iteration 54, loss = 0.01377156\n",
      "Iteration 55, loss = 0.01424767\n",
      "Iteration 56, loss = 0.01746373\n",
      "Iteration 57, loss = 0.01439430\n",
      "Iteration 58, loss = 0.01319229\n",
      "Iteration 59, loss = 0.01402690\n",
      "Iteration 60, loss = 0.01424382\n",
      "Iteration 61, loss = 0.02091159\n",
      "Iteration 62, loss = 0.01659727\n",
      "Iteration 63, loss = 0.01669524\n",
      "Iteration 64, loss = 0.01386090\n",
      "Iteration 65, loss = 0.01294405\n",
      "Iteration 66, loss = 0.01263194\n",
      "Iteration 67, loss = 0.02724138\n",
      "Iteration 68, loss = 0.01303231\n",
      "Iteration 69, loss = 0.01292196\n",
      "Iteration 70, loss = 0.01219772\n",
      "Iteration 71, loss = 0.01202679\n",
      "Iteration 72, loss = 0.01183018\n",
      "Iteration 73, loss = 0.01151628\n",
      "Iteration 74, loss = 0.01149055\n",
      "Iteration 75, loss = 0.01491615\n",
      "Iteration 76, loss = 0.01155718\n",
      "Iteration 77, loss = 0.01093403\n",
      "Iteration 78, loss = 0.01394764\n",
      "Iteration 79, loss = 0.01189051\n",
      "Iteration 80, loss = 0.01210025\n",
      "Iteration 81, loss = 0.01237976\n",
      "Iteration 82, loss = 0.01370664\n",
      "Iteration 83, loss = 0.01246402\n",
      "Iteration 84, loss = 0.02993329\n",
      "Iteration 85, loss = 0.01055846\n",
      "Iteration 86, loss = 0.01027584\n",
      "Iteration 87, loss = 0.01094254\n",
      "Iteration 88, loss = 0.00988806\n",
      "Iteration 89, loss = 0.01161955\n",
      "Iteration 90, loss = 0.01052196\n",
      "Iteration 91, loss = 0.01104565\n",
      "Iteration 92, loss = 0.01059746\n",
      "Iteration 93, loss = 0.01125321\n",
      "Iteration 94, loss = 0.01120117\n",
      "Iteration 95, loss = 0.01322706\n",
      "Iteration 96, loss = 0.01062074\n",
      "Iteration 97, loss = 0.00947486\n",
      "Iteration 98, loss = 0.00988814\n",
      "Iteration 99, loss = 0.01647220\n",
      "Iteration 100, loss = 0.01106361\n",
      "Iteration 101, loss = 0.01140965\n",
      "Iteration 102, loss = 0.01214959\n",
      "Iteration 103, loss = 0.00888108\n",
      "Iteration 104, loss = 0.00881171\n",
      "Iteration 105, loss = 0.01009654\n",
      "Iteration 106, loss = 0.00998920\n",
      "Iteration 107, loss = 0.01048154\n",
      "Iteration 108, loss = 0.00977419\n",
      "Iteration 109, loss = 0.00887449\n",
      "Iteration 110, loss = 0.01240393\n",
      "Iteration 111, loss = 0.01236069\n",
      "Iteration 112, loss = 0.01072246\n",
      "Iteration 113, loss = 0.00912593\n",
      "Iteration 114, loss = 0.00955052\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "# vanilla classification\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_embeddings, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fft classification with sliding windows\n",
    "\n",
    "from split_utils import split_text\n",
    "\n",
    "all_docs_paras = [split_text(doc, segment_length=40) for doc in all_docs]\n",
    "\n",
    "# remove any empty paragraphs\n",
    "all_docs_paras = [[para for para in paras if para] for paras in all_docs_paras]\n",
    "# remove any '' paragraphs\n",
    "all_docs_paras = [[para for para in paras if para != ''] for paras in all_docs_paras]\n",
    "\n",
    "# get embeddings for each paragraph\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "train_embeddings_paras = [get_embeddings(paras, engine=embeddings_engine) for paras in tqdm(all_docs_paras)]\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'wb') as f:\n",
    "    pickle.dump(train_embeddings_paras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [00:04<00:00, 227.52it/s]\n",
      "100%|██████████| 1063/1063 [00:21<00:00, 50.27it/s]\n",
      "100%|██████████| 1063/1063 [00:11<00:00, 95.86it/s] \n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'rb') as f:\n",
    "    all_embeddings_paras = pickle.load(f)\n",
    "\n",
    "# convert to numpy arrays\n",
    "all_embeddings_paras = [np.array(doc) for doc in all_embeddings_paras]\n",
    "\n",
    "# get FFTs\n",
    "def get_fft(embedding):\n",
    "    return librosa.stft(embedding, n_fft=32, win_length=4)\n",
    "\n",
    "# lowpass filter\n",
    "def lowpass_filter(fft, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Lowpass filter for FFTs\n",
    "    \"\"\"\n",
    "    fft = fft.copy()\n",
    "    fft[:, int(cutoff*fft.shape[1]):] = 0\n",
    "    return fft\n",
    "\n",
    "# convert back to embeddings\n",
    "def fft_to_embedding(fft):\n",
    "    return librosa.istft(fft, win_length=4)\n",
    "\n",
    "apply_lowpass = True\n",
    "\n",
    "# get FFTs\n",
    "all_embeddings_paras_fft = [get_fft(embedding) for embedding in tqdm(all_embeddings_paras)]\n",
    "\n",
    "if apply_lowpass:\n",
    "    # lowpass filter\n",
    "    all_embeddings_paras_fft = [lowpass_filter(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "# convert back to embeddings\n",
    "all_embeddings_paras_lowpass = [fft_to_embedding(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "if not apply_lowpass:\n",
    "    # assert that the embeddings are the same if lowpass filtering is not applied\n",
    "    assert np.allclose(all_embeddings_paras_lowpass[0], all_embeddings_paras[0])\n",
    "\n",
    "# average embeddings\n",
    "train_embeddings_lowpass_avg = [np.mean(embeddings, axis=0) for embeddings in all_embeddings_paras_lowpass]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70154922\n",
      "Iteration 2, loss = 0.68757508\n",
      "Iteration 3, loss = 0.69115338\n",
      "Iteration 4, loss = 0.68634688\n",
      "Iteration 5, loss = 0.68233397\n",
      "Iteration 6, loss = 0.67981871\n",
      "Iteration 7, loss = 0.67443565\n",
      "Iteration 8, loss = 0.66437422\n",
      "Iteration 9, loss = 0.65246965\n",
      "Iteration 10, loss = 0.63410225\n",
      "Iteration 11, loss = 0.59864546\n",
      "Iteration 12, loss = 0.54434998\n",
      "Iteration 13, loss = 0.46970671\n",
      "Iteration 14, loss = 0.36388840\n",
      "Iteration 15, loss = 0.24746705\n",
      "Iteration 16, loss = 0.20432111\n",
      "Iteration 17, loss = 1.03771850\n",
      "Iteration 18, loss = 0.12731552\n",
      "Iteration 19, loss = 0.09482040\n",
      "Iteration 20, loss = 0.07808300\n",
      "Iteration 21, loss = 0.06601256\n",
      "Iteration 22, loss = 0.06254007\n",
      "Iteration 23, loss = 0.06556396\n",
      "Iteration 24, loss = 0.06459648\n",
      "Iteration 25, loss = 0.05485990\n",
      "Iteration 26, loss = 0.05022966\n",
      "Iteration 27, loss = 0.05301977\n",
      "Iteration 28, loss = 0.04676607\n",
      "Iteration 29, loss = 0.04580099\n",
      "Iteration 30, loss = 0.04536240\n",
      "Iteration 31, loss = 0.04479018\n",
      "Iteration 32, loss = 0.04575981\n",
      "Iteration 33, loss = 0.04226844\n",
      "Iteration 34, loss = 0.04145817\n",
      "Iteration 35, loss = 0.04071518\n",
      "Iteration 36, loss = 0.03943636\n",
      "Iteration 37, loss = 0.04331634\n",
      "Iteration 38, loss = 0.03901809\n",
      "Iteration 39, loss = 0.03903391\n",
      "Iteration 40, loss = 0.03892689\n",
      "Iteration 41, loss = 0.04009750\n",
      "Iteration 42, loss = 0.03968606\n",
      "Iteration 43, loss = 0.05203427\n",
      "Iteration 44, loss = 0.03980418\n",
      "Iteration 45, loss = 0.04544134\n",
      "Iteration 46, loss = 0.03678114\n",
      "Iteration 47, loss = 0.04231588\n",
      "Iteration 48, loss = 0.03635698\n",
      "Iteration 49, loss = 0.03563679\n",
      "Iteration 50, loss = 0.03497642\n",
      "Iteration 51, loss = 0.03520761\n",
      "Iteration 52, loss = 0.03512114\n",
      "Iteration 53, loss = 0.03547770\n",
      "Iteration 54, loss = 0.03294916\n",
      "Iteration 55, loss = 0.03389028\n",
      "Iteration 56, loss = 0.03870995\n",
      "Iteration 57, loss = 0.03373441\n",
      "Iteration 58, loss = 0.03159874\n",
      "Iteration 59, loss = 0.03433115\n",
      "Iteration 60, loss = 0.03277829\n",
      "Iteration 61, loss = 0.03544698\n",
      "Iteration 62, loss = 0.03403032\n",
      "Iteration 63, loss = 0.03562315\n",
      "Iteration 64, loss = 0.03123255\n",
      "Iteration 65, loss = 0.03026992\n",
      "Iteration 66, loss = 0.03084709\n",
      "Iteration 67, loss = 0.04469303\n",
      "Iteration 68, loss = 0.03034080\n",
      "Iteration 69, loss = 0.03650696\n",
      "Iteration 70, loss = 0.03300519\n",
      "Iteration 71, loss = 0.03262852\n",
      "Iteration 72, loss = 0.02978147\n",
      "Iteration 73, loss = 0.03357906\n",
      "Iteration 74, loss = 0.03111084\n",
      "Iteration 75, loss = 0.03516103\n",
      "Iteration 76, loss = 0.03341043\n",
      "Iteration 77, loss = 0.03756768\n",
      "Iteration 78, loss = 0.02919445\n",
      "Iteration 79, loss = 0.02924371\n",
      "Iteration 80, loss = 0.02924141\n",
      "Iteration 81, loss = 0.02805666\n",
      "Iteration 82, loss = 0.02850822\n",
      "Iteration 83, loss = 0.02994857\n",
      "Iteration 84, loss = 0.04597034\n",
      "Iteration 85, loss = 0.02746794\n",
      "Iteration 86, loss = 0.02763158\n",
      "Iteration 87, loss = 0.02615622\n",
      "Iteration 88, loss = 0.02614562\n",
      "Iteration 89, loss = 0.02581473\n",
      "Iteration 90, loss = 0.02596937\n",
      "Iteration 91, loss = 0.03110881\n",
      "Iteration 92, loss = 0.02535205\n",
      "Iteration 93, loss = 0.02490367\n",
      "Iteration 94, loss = 0.02837000\n",
      "Iteration 95, loss = 0.03024165\n",
      "Iteration 96, loss = 0.02989861\n",
      "Iteration 97, loss = 0.02638977\n",
      "Iteration 98, loss = 0.02419586\n",
      "Iteration 99, loss = 0.03883585\n",
      "Iteration 100, loss = 0.02627658\n",
      "Iteration 101, loss = 0.03226361\n",
      "Iteration 102, loss = 0.03239005\n",
      "Iteration 103, loss = 0.02447815\n",
      "Iteration 104, loss = 0.02475437\n",
      "Iteration 105, loss = 0.02584472\n",
      "Iteration 106, loss = 0.02571330\n",
      "Iteration 107, loss = 0.02188799\n",
      "Iteration 108, loss = 0.02286482\n",
      "Iteration 109, loss = 0.02236644\n",
      "Iteration 110, loss = 0.03318039\n",
      "Iteration 111, loss = 0.02248993\n",
      "Iteration 112, loss = 0.02738014\n",
      "Iteration 113, loss = 0.02234861\n",
      "Iteration 114, loss = 0.02141778\n",
      "Iteration 115, loss = 0.02725785\n",
      "Iteration 116, loss = 0.02129402\n",
      "Iteration 117, loss = 0.02936049\n",
      "Iteration 118, loss = 0.02089623\n",
      "Iteration 119, loss = 0.02349847\n",
      "Iteration 120, loss = 0.02278427\n",
      "Iteration 121, loss = 0.02224469\n",
      "Iteration 122, loss = 0.02050003\n",
      "Iteration 123, loss = 0.03239895\n",
      "Iteration 124, loss = 0.03149340\n",
      "Iteration 125, loss = 0.02287599\n",
      "Iteration 126, loss = 0.02035503\n",
      "Iteration 127, loss = 0.01976054\n",
      "Iteration 128, loss = 0.03565151\n",
      "Iteration 129, loss = 0.02078843\n",
      "Iteration 130, loss = 0.02038910\n",
      "Iteration 131, loss = 0.02163490\n",
      "Iteration 132, loss = 0.02025868\n",
      "Iteration 133, loss = 0.02088090\n",
      "Iteration 134, loss = 0.02103029\n",
      "Iteration 135, loss = 0.02405121\n",
      "Iteration 136, loss = 0.02400226\n",
      "Iteration 137, loss = 0.01948751\n",
      "Iteration 138, loss = 0.03080060\n",
      "Iteration 139, loss = 0.01858837\n",
      "Iteration 140, loss = 0.02737162\n",
      "Iteration 141, loss = 0.02765123\n",
      "Iteration 142, loss = 0.02178586\n",
      "Iteration 143, loss = 0.02149999\n",
      "Iteration 144, loss = 0.01813290\n",
      "Iteration 145, loss = 0.02706030\n",
      "Iteration 146, loss = 0.02145396\n",
      "Iteration 147, loss = 0.01914858\n",
      "Iteration 148, loss = 0.02579018\n",
      "Iteration 149, loss = 0.01901086\n",
      "Iteration 150, loss = 0.01835455\n",
      "Iteration 151, loss = 0.02232610\n",
      "Iteration 152, loss = 0.01925601\n",
      "Iteration 153, loss = 0.02843658\n",
      "Iteration 154, loss = 0.03216015\n",
      "Iteration 155, loss = 0.01666037\n",
      "Iteration 156, loss = 0.01962967\n",
      "Iteration 157, loss = 0.01681486\n",
      "Iteration 158, loss = 0.02001706\n",
      "Iteration 159, loss = 0.01671772\n",
      "Iteration 160, loss = 0.02166910\n",
      "Iteration 161, loss = 0.01771232\n",
      "Iteration 162, loss = 0.02195417\n",
      "Iteration 163, loss = 0.01776902\n",
      "Iteration 164, loss = 0.01770542\n",
      "Iteration 165, loss = 0.01724514\n",
      "Iteration 166, loss = 0.01680731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.9671361502347418\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_embeddings_lowpass_avg, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results\n",
    "\n",
    "- get embeddings for whole text: 97.1% accuracy\n",
    "- sliding window without lowpass filter: 96% accuracy\n",
    "- sliding window with lowpass filter @ 0.5: 97.6% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e67b88d6ad197c9402ec873fcbcf9f15e38aeaf8485f1023dc62e85e716efbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
