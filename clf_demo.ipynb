{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# document classification demo for spectral embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk openai tqdm pandas numpy matplotlib librosa soundfile plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "with open('/users/jasper/oai.txt', 'r') as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "# nltk.download('reuters')\n",
    "\n",
    "trade_docs = reuters.fileids(categories='trade')\n",
    "crude_docs = reuters.fileids(categories='crude')\n",
    "\n",
    "all_docs = [reuters.raw(doc_id) for doc_id in trade_docs + crude_docs]\n",
    "all_labels = ['trade' for _ in trade_docs] + ['crude' for _ in crude_docs]\n",
    "\n",
    "# shuffle docs and labels together\n",
    "np.random.seed(42)\n",
    "combined = list(zip(all_docs, all_labels))\n",
    "np.random.shuffle(combined)\n",
    "all_docs, all_labels = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for train/test docs\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "all_embeddings = get_embeddings(all_docs, engine=embeddings_engine)\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'rb') as f:\n",
    "    all_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70101367\n",
      "Iteration 2, loss = 0.68367065\n",
      "Iteration 3, loss = 0.68158052\n",
      "Iteration 4, loss = 0.66672913\n",
      "Iteration 5, loss = 0.64261986\n",
      "Iteration 6, loss = 0.59763393\n",
      "Iteration 7, loss = 0.50344354\n",
      "Iteration 8, loss = 0.34982035\n",
      "Iteration 9, loss = 0.18737424\n",
      "Iteration 10, loss = 0.09429573\n",
      "Iteration 11, loss = 0.06038294\n",
      "Iteration 12, loss = 0.04866964\n",
      "Iteration 13, loss = 0.04410997\n",
      "Iteration 14, loss = 0.04180204\n",
      "Iteration 15, loss = 0.04022787\n",
      "Iteration 16, loss = 0.03893421\n",
      "Iteration 17, loss = 0.03733961\n",
      "Iteration 18, loss = 0.03657173\n",
      "Iteration 19, loss = 0.03535737\n",
      "Iteration 20, loss = 0.03205988\n",
      "Iteration 21, loss = 0.03007697\n",
      "Iteration 22, loss = 0.03237719\n",
      "Iteration 23, loss = 0.03248144\n",
      "Iteration 24, loss = 0.03481693\n",
      "Iteration 25, loss = 0.02949732\n",
      "Iteration 26, loss = 0.02617889\n",
      "Iteration 27, loss = 0.02856872\n",
      "Iteration 28, loss = 0.02560701\n",
      "Iteration 29, loss = 0.02881651\n",
      "Iteration 30, loss = 0.02308987\n",
      "Iteration 31, loss = 0.02350713\n",
      "Iteration 32, loss = 0.02478771\n",
      "Iteration 33, loss = 0.02227817\n",
      "Iteration 34, loss = 0.02207347\n",
      "Iteration 35, loss = 0.02317622\n",
      "Iteration 36, loss = 0.02158565\n",
      "Iteration 37, loss = 0.02364045\n",
      "Iteration 38, loss = 0.01880637\n",
      "Iteration 39, loss = 0.01758825\n",
      "Iteration 40, loss = 0.01852360\n",
      "Iteration 41, loss = 0.01759473\n",
      "Iteration 42, loss = 0.01783700\n",
      "Iteration 43, loss = 0.02299533\n",
      "Iteration 44, loss = 0.01727168\n",
      "Iteration 45, loss = 0.01813288\n",
      "Iteration 46, loss = 0.01666761\n",
      "Iteration 47, loss = 0.02003449\n",
      "Iteration 48, loss = 0.01504644\n",
      "Iteration 49, loss = 0.01478703\n",
      "Iteration 50, loss = 0.01576593\n",
      "Iteration 51, loss = 0.01540480\n",
      "Iteration 52, loss = 0.01541519\n",
      "Iteration 53, loss = 0.01465030\n",
      "Iteration 54, loss = 0.01377156\n",
      "Iteration 55, loss = 0.01424767\n",
      "Iteration 56, loss = 0.01746373\n",
      "Iteration 57, loss = 0.01439430\n",
      "Iteration 58, loss = 0.01319229\n",
      "Iteration 59, loss = 0.01402690\n",
      "Iteration 60, loss = 0.01424382\n",
      "Iteration 61, loss = 0.02091159\n",
      "Iteration 62, loss = 0.01659727\n",
      "Iteration 63, loss = 0.01669524\n",
      "Iteration 64, loss = 0.01386090\n",
      "Iteration 65, loss = 0.01294405\n",
      "Iteration 66, loss = 0.01263194\n",
      "Iteration 67, loss = 0.02724138\n",
      "Iteration 68, loss = 0.01303231\n",
      "Iteration 69, loss = 0.01292196\n",
      "Iteration 70, loss = 0.01219772\n",
      "Iteration 71, loss = 0.01202679\n",
      "Iteration 72, loss = 0.01183018\n",
      "Iteration 73, loss = 0.01151628\n",
      "Iteration 74, loss = 0.01149055\n",
      "Iteration 75, loss = 0.01491615\n",
      "Iteration 76, loss = 0.01155718\n",
      "Iteration 77, loss = 0.01093403\n",
      "Iteration 78, loss = 0.01394764\n",
      "Iteration 79, loss = 0.01189051\n",
      "Iteration 80, loss = 0.01210025\n",
      "Iteration 81, loss = 0.01237976\n",
      "Iteration 82, loss = 0.01370664\n",
      "Iteration 83, loss = 0.01246402\n",
      "Iteration 84, loss = 0.02993329\n",
      "Iteration 85, loss = 0.01055846\n",
      "Iteration 86, loss = 0.01027584\n",
      "Iteration 87, loss = 0.01094254\n",
      "Iteration 88, loss = 0.00988806\n",
      "Iteration 89, loss = 0.01161955\n",
      "Iteration 90, loss = 0.01052196\n",
      "Iteration 91, loss = 0.01104565\n",
      "Iteration 92, loss = 0.01059746\n",
      "Iteration 93, loss = 0.01125321\n",
      "Iteration 94, loss = 0.01120117\n",
      "Iteration 95, loss = 0.01322706\n",
      "Iteration 96, loss = 0.01062074\n",
      "Iteration 97, loss = 0.00947486\n",
      "Iteration 98, loss = 0.00988814\n",
      "Iteration 99, loss = 0.01647220\n",
      "Iteration 100, loss = 0.01106361\n",
      "Iteration 101, loss = 0.01140965\n",
      "Iteration 102, loss = 0.01214959\n",
      "Iteration 103, loss = 0.00888108\n",
      "Iteration 104, loss = 0.00881171\n",
      "Iteration 105, loss = 0.01009654\n",
      "Iteration 106, loss = 0.00998920\n",
      "Iteration 107, loss = 0.01048154\n",
      "Iteration 108, loss = 0.00977419\n",
      "Iteration 109, loss = 0.00887449\n",
      "Iteration 110, loss = 0.01240393\n",
      "Iteration 111, loss = 0.01236069\n",
      "Iteration 112, loss = 0.01072246\n",
      "Iteration 113, loss = 0.00912593\n",
      "Iteration 114, loss = 0.00955052\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "# vanilla classification\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_embeddings, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fft classification with sliding windows\n",
    "\n",
    "from split_utils import split_text\n",
    "\n",
    "all_docs_paras = [split_text(doc, segment_length=40) for doc in all_docs]\n",
    "\n",
    "# remove any empty paragraphs\n",
    "all_docs_paras = [[para for para in paras if para] for paras in all_docs_paras]\n",
    "# remove any '' paragraphs\n",
    "all_docs_paras = [[para for para in paras if para != ''] for paras in all_docs_paras]\n",
    "\n",
    "# get embeddings for each paragraph\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "train_embeddings_paras = [get_embeddings(paras, engine=embeddings_engine) for paras in tqdm(all_docs_paras)]\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'wb') as f:\n",
    "    pickle.dump(train_embeddings_paras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [00:06<00:00, 162.81it/s]\n",
      "100%|██████████| 1063/1063 [00:41<00:00, 25.85it/s]\n",
      "100%|██████████| 1063/1063 [00:24<00:00, 43.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70159241\n",
      "Iteration 2, loss = 0.68765397\n",
      "Iteration 3, loss = 0.69103178\n",
      "Iteration 4, loss = 0.68673264\n",
      "Iteration 5, loss = 0.68363094\n",
      "Iteration 6, loss = 0.68185168\n",
      "Iteration 7, loss = 0.67816263\n",
      "Iteration 8, loss = 0.67072681\n",
      "Iteration 9, loss = 0.66360982\n",
      "Iteration 10, loss = 0.65299848\n",
      "Iteration 11, loss = 0.63267743\n",
      "Iteration 12, loss = 0.60291320\n",
      "Iteration 13, loss = 0.55914128\n",
      "Iteration 14, loss = 0.49789034\n",
      "Iteration 15, loss = 0.39699174\n",
      "Iteration 16, loss = 0.29906953\n",
      "Iteration 17, loss = 0.71127917\n",
      "Iteration 18, loss = 0.49765811\n",
      "Iteration 19, loss = 0.14356635\n",
      "Iteration 20, loss = 0.11358551\n",
      "Iteration 21, loss = 0.09014602\n",
      "Iteration 22, loss = 0.07832576\n",
      "Iteration 23, loss = 0.08047528\n",
      "Iteration 24, loss = 0.08414863\n",
      "Iteration 25, loss = 0.06992368\n",
      "Iteration 26, loss = 0.05706005\n",
      "Iteration 27, loss = 0.05756711\n",
      "Iteration 28, loss = 0.05097235\n",
      "Iteration 29, loss = 0.04987653\n",
      "Iteration 30, loss = 0.04936011\n",
      "Iteration 31, loss = 0.04708928\n",
      "Iteration 32, loss = 0.05021702\n",
      "Iteration 33, loss = 0.04406526\n",
      "Iteration 34, loss = 0.04413398\n",
      "Iteration 35, loss = 0.04269739\n",
      "Iteration 36, loss = 0.04118694\n",
      "Iteration 37, loss = 0.04589864\n",
      "Iteration 38, loss = 0.04054855\n",
      "Iteration 39, loss = 0.04089553\n",
      "Iteration 40, loss = 0.04113089\n",
      "Iteration 41, loss = 0.04293571\n",
      "Iteration 42, loss = 0.04236597\n",
      "Iteration 43, loss = 0.05851888\n",
      "Iteration 44, loss = 0.04035307\n",
      "Iteration 45, loss = 0.05293357\n",
      "Iteration 46, loss = 0.03901247\n",
      "Iteration 47, loss = 0.04461792\n",
      "Iteration 48, loss = 0.03922502\n",
      "Iteration 49, loss = 0.03773531\n",
      "Iteration 50, loss = 0.03680414\n",
      "Iteration 51, loss = 0.03699886\n",
      "Iteration 52, loss = 0.03863598\n",
      "Iteration 53, loss = 0.03691343\n",
      "Iteration 54, loss = 0.03467746\n",
      "Iteration 55, loss = 0.03592357\n",
      "Iteration 56, loss = 0.04252833\n",
      "Iteration 57, loss = 0.03522705\n",
      "Iteration 58, loss = 0.03277174\n",
      "Iteration 59, loss = 0.03624020\n",
      "Iteration 60, loss = 0.03460041\n",
      "Iteration 61, loss = 0.03756096\n",
      "Iteration 62, loss = 0.03641842\n",
      "Iteration 63, loss = 0.03712313\n",
      "Iteration 64, loss = 0.03283216\n",
      "Iteration 65, loss = 0.03183345\n",
      "Iteration 66, loss = 0.03239826\n",
      "Iteration 67, loss = 0.04804410\n",
      "Iteration 68, loss = 0.03183387\n",
      "Iteration 69, loss = 0.03913283\n",
      "Iteration 70, loss = 0.03412121\n",
      "Iteration 71, loss = 0.03525999\n",
      "Iteration 72, loss = 0.03153695\n",
      "Iteration 73, loss = 0.03754112\n",
      "Iteration 74, loss = 0.03463758\n",
      "Iteration 75, loss = 0.04826251\n",
      "Iteration 76, loss = 0.03432699\n",
      "Iteration 77, loss = 0.04079003\n",
      "Iteration 78, loss = 0.03137511\n",
      "Iteration 79, loss = 0.03094452\n",
      "Iteration 80, loss = 0.03113365\n",
      "Iteration 81, loss = 0.02992602\n",
      "Iteration 82, loss = 0.03064831\n",
      "Iteration 83, loss = 0.03221408\n",
      "Iteration 84, loss = 0.05018036\n",
      "Iteration 85, loss = 0.02991032\n",
      "Iteration 86, loss = 0.03005296\n",
      "Iteration 87, loss = 0.02803100\n",
      "Iteration 88, loss = 0.02790700\n",
      "Iteration 89, loss = 0.02723967\n",
      "Iteration 90, loss = 0.02756880\n",
      "Iteration 91, loss = 0.03394307\n",
      "Iteration 92, loss = 0.02692338\n",
      "Iteration 93, loss = 0.02645932\n",
      "Iteration 94, loss = 0.03136839\n",
      "Iteration 95, loss = 0.03337724\n",
      "Iteration 96, loss = 0.03294612\n",
      "Iteration 97, loss = 0.02846911\n",
      "Iteration 98, loss = 0.02609099\n",
      "Iteration 99, loss = 0.04741415\n",
      "Iteration 100, loss = 0.03171777\n",
      "Iteration 101, loss = 0.03905280\n",
      "Iteration 102, loss = 0.03674920\n",
      "Iteration 103, loss = 0.02691623\n",
      "Iteration 104, loss = 0.02649030\n",
      "Iteration 105, loss = 0.02802547\n",
      "Iteration 106, loss = 0.02786840\n",
      "Iteration 107, loss = 0.02330816\n",
      "Iteration 108, loss = 0.02456730\n",
      "Iteration 109, loss = 0.02406220\n",
      "Iteration 110, loss = 0.04001391\n",
      "Iteration 111, loss = 0.02463938\n",
      "Iteration 112, loss = 0.03433175\n",
      "Iteration 113, loss = 0.02679206\n",
      "Iteration 114, loss = 0.02258160\n",
      "Iteration 115, loss = 0.02993308\n",
      "Iteration 116, loss = 0.02277629\n",
      "Iteration 117, loss = 0.03027047\n",
      "Iteration 118, loss = 0.02247690\n",
      "Iteration 119, loss = 0.02534493\n",
      "Iteration 120, loss = 0.02478194\n",
      "Iteration 121, loss = 0.02439779\n",
      "Iteration 122, loss = 0.02258888\n",
      "Iteration 123, loss = 0.03959799\n",
      "Iteration 124, loss = 0.03042315\n",
      "Iteration 125, loss = 0.02554389\n",
      "Iteration 126, loss = 0.02211898\n",
      "Iteration 127, loss = 0.02181152\n",
      "Iteration 128, loss = 0.04767032\n",
      "Iteration 129, loss = 0.02351003\n",
      "Iteration 130, loss = 0.02268407\n",
      "Iteration 131, loss = 0.02302730\n",
      "Iteration 132, loss = 0.02205369\n",
      "Iteration 133, loss = 0.02232915\n",
      "Iteration 134, loss = 0.02283160\n",
      "Iteration 135, loss = 0.02558702\n",
      "Iteration 136, loss = 0.02715314\n",
      "Iteration 137, loss = 0.02128914\n",
      "Iteration 138, loss = 0.03613673\n",
      "Iteration 139, loss = 0.01973363\n",
      "Iteration 140, loss = 0.03067881\n",
      "Iteration 141, loss = 0.03172887\n",
      "Iteration 142, loss = 0.02489206\n",
      "Iteration 143, loss = 0.02346456\n",
      "Iteration 144, loss = 0.02017191\n",
      "Iteration 145, loss = 0.02986788\n",
      "Iteration 146, loss = 0.02452654\n",
      "Iteration 147, loss = 0.02285100\n",
      "Iteration 148, loss = 0.02884556\n",
      "Iteration 149, loss = 0.02029832\n",
      "Iteration 150, loss = 0.02042466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'rb') as f:\n",
    "    all_embeddings_paras = pickle.load(f)\n",
    "\n",
    "# convert to numpy arrays\n",
    "all_embeddings_paras = [np.array(doc) for doc in all_embeddings_paras]\n",
    "\n",
    "# get FFTs\n",
    "def get_fft(embedding):\n",
    "    return librosa.stft(embedding, n_fft=32, win_length=4)\n",
    "\n",
    "# lowpass filter\n",
    "def lowpass_filter(fft, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Lowpass filter for FFTs\n",
    "    \"\"\"\n",
    "    fft = fft.copy()\n",
    "    fft[:, int(cutoff*fft.shape[1]):] = 0\n",
    "    return fft\n",
    "\n",
    "# convert back to embeddings\n",
    "def fft_to_embedding(fft):\n",
    "    return librosa.istft(fft, win_length=4)\n",
    "\n",
    "apply_lowpass = True\n",
    "\n",
    "# get FFTs\n",
    "all_embeddings_paras_fft = [get_fft(embedding) for embedding in tqdm(all_embeddings_paras)]\n",
    "\n",
    "if apply_lowpass:\n",
    "    # lowpass filter\n",
    "    all_embeddings_paras_fft = [lowpass_filter(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "# convert back to embeddings\n",
    "all_embeddings_paras_lowpass = [fft_to_embedding(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "if not apply_lowpass:\n",
    "    # assert that the embeddings are the same if lowpass filtering is not applied\n",
    "    assert np.allclose(all_embeddings_paras_lowpass[0], all_embeddings_paras[0])\n",
    "\n",
    "# average embeddings\n",
    "train_embeddings_lowpass_avg = [np.mean(embeddings, axis=0) for embeddings in all_embeddings_paras_lowpass]\n",
    "\n",
    "# train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_embeddings_lowpass_avg, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results\n",
    "\n",
    "- get embeddings for whole text: 97.1% accuracy\n",
    "- sliding window without lowpass filter: 96% accuracy\n",
    "- sliding window with lowpass filter @ 0.5: 97.6% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e67b88d6ad197c9402ec873fcbcf9f15e38aeaf8485f1023dc62e85e716efbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
