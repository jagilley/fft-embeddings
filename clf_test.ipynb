{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: openai in ./.venv/lib/python3.10/site-packages (0.27.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: librosa in ./.venv/lib/python3.10/site-packages (0.10.0)\n",
      "Requirement already satisfied: soundfile in ./.venv/lib/python3.10/site-packages (0.12.1)\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.13.1-py2.py3-none-any.whl (15.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: requests>=2.20 in ./.venv/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./.venv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./.venv/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.2.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in ./.venv/lib/python3.10/site-packages (from librosa) (0.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in ./.venv/lib/python3.10/site-packages (from librosa) (0.3.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.0.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./.venv/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: pooch>=1.0 in ./.venv/lib/python3.10/site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in ./.venv/lib/python3.10/site-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in ./.venv/lib/python3.10/site-packages (from librosa) (0.56.4)\n",
      "Requirement already satisfied: audioread>=2.1.9 in ./.venv/lib/python3.10/site-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.10/site-packages (from soundfile) (1.15.1)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (65.5.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in ./.venv/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in ./.venv/lib/python3.10/site-packages (from pooch>=1.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.20->openai) (3.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.venv/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Installing collected packages: tenacity, plotly\n",
      "Successfully installed plotly-5.13.1 tenacity-8.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk openai tqdm pandas numpy matplotlib librosa soundfile plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import openai\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import nltk\n",
    "from openai.embeddings_utils import get_embeddings\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "with open('/users/jasper/oai.txt', 'r') as f:\n",
    "    openai.api_key = f.read()\n",
    "\n",
    "# nltk.download('reuters')\n",
    "\n",
    "trade_docs = reuters.fileids(categories='trade')\n",
    "crude_docs = reuters.fileids(categories='crude')\n",
    "\n",
    "all_docs = [reuters.raw(doc_id) for doc_id in trade_docs + crude_docs]\n",
    "all_labels = ['trade' for _ in trade_docs] + ['crude' for _ in crude_docs]\n",
    "\n",
    "# shuffle docs and labels together\n",
    "np.random.seed(42)\n",
    "combined = list(zip(all_docs, all_labels))\n",
    "np.random.shuffle(combined)\n",
    "all_docs, all_labels = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for train/test docs\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "all_embeddings = get_embeddings(all_docs, engine=embeddings_engine)\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings.pkl', 'rb') as f:\n",
    "    all_embeddings = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70101367\n",
      "Iteration 2, loss = 0.68367065\n",
      "Iteration 3, loss = 0.68158052\n",
      "Iteration 4, loss = 0.66672913\n",
      "Iteration 5, loss = 0.64261986\n",
      "Iteration 6, loss = 0.59763393\n",
      "Iteration 7, loss = 0.50344354\n",
      "Iteration 8, loss = 0.34982035\n",
      "Iteration 9, loss = 0.18737424\n",
      "Iteration 10, loss = 0.09429573\n",
      "Iteration 11, loss = 0.06038294\n",
      "Iteration 12, loss = 0.04866964\n",
      "Iteration 13, loss = 0.04410997\n",
      "Iteration 14, loss = 0.04180204\n",
      "Iteration 15, loss = 0.04022787\n",
      "Iteration 16, loss = 0.03893421\n",
      "Iteration 17, loss = 0.03733961\n",
      "Iteration 18, loss = 0.03657173\n",
      "Iteration 19, loss = 0.03535737\n",
      "Iteration 20, loss = 0.03205988\n",
      "Iteration 21, loss = 0.03007697\n",
      "Iteration 22, loss = 0.03237719\n",
      "Iteration 23, loss = 0.03248144\n",
      "Iteration 24, loss = 0.03481693\n",
      "Iteration 25, loss = 0.02949732\n",
      "Iteration 26, loss = 0.02617889\n",
      "Iteration 27, loss = 0.02856872\n",
      "Iteration 28, loss = 0.02560701\n",
      "Iteration 29, loss = 0.02881651\n",
      "Iteration 30, loss = 0.02308987\n",
      "Iteration 31, loss = 0.02350713\n",
      "Iteration 32, loss = 0.02478771\n",
      "Iteration 33, loss = 0.02227817\n",
      "Iteration 34, loss = 0.02207347\n",
      "Iteration 35, loss = 0.02317622\n",
      "Iteration 36, loss = 0.02158565\n",
      "Iteration 37, loss = 0.02364045\n",
      "Iteration 38, loss = 0.01880637\n",
      "Iteration 39, loss = 0.01758825\n",
      "Iteration 40, loss = 0.01852360\n",
      "Iteration 41, loss = 0.01759473\n",
      "Iteration 42, loss = 0.01783700\n",
      "Iteration 43, loss = 0.02299533\n",
      "Iteration 44, loss = 0.01727168\n",
      "Iteration 45, loss = 0.01813288\n",
      "Iteration 46, loss = 0.01666761\n",
      "Iteration 47, loss = 0.02003449\n",
      "Iteration 48, loss = 0.01504644\n",
      "Iteration 49, loss = 0.01478703\n",
      "Iteration 50, loss = 0.01576593\n",
      "Iteration 51, loss = 0.01540480\n",
      "Iteration 52, loss = 0.01541519\n",
      "Iteration 53, loss = 0.01465030\n",
      "Iteration 54, loss = 0.01377156\n",
      "Iteration 55, loss = 0.01424767\n",
      "Iteration 56, loss = 0.01746373\n",
      "Iteration 57, loss = 0.01439430\n",
      "Iteration 58, loss = 0.01319229\n",
      "Iteration 59, loss = 0.01402690\n",
      "Iteration 60, loss = 0.01424382\n",
      "Iteration 61, loss = 0.02091159\n",
      "Iteration 62, loss = 0.01659727\n",
      "Iteration 63, loss = 0.01669524\n",
      "Iteration 64, loss = 0.01386090\n",
      "Iteration 65, loss = 0.01294405\n",
      "Iteration 66, loss = 0.01263194\n",
      "Iteration 67, loss = 0.02724138\n",
      "Iteration 68, loss = 0.01303231\n",
      "Iteration 69, loss = 0.01292196\n",
      "Iteration 70, loss = 0.01219772\n",
      "Iteration 71, loss = 0.01202679\n",
      "Iteration 72, loss = 0.01183018\n",
      "Iteration 73, loss = 0.01151628\n",
      "Iteration 74, loss = 0.01149055\n",
      "Iteration 75, loss = 0.01491615\n",
      "Iteration 76, loss = 0.01155718\n",
      "Iteration 77, loss = 0.01093403\n",
      "Iteration 78, loss = 0.01394764\n",
      "Iteration 79, loss = 0.01189051\n",
      "Iteration 80, loss = 0.01210025\n",
      "Iteration 81, loss = 0.01237976\n",
      "Iteration 82, loss = 0.01370664\n",
      "Iteration 83, loss = 0.01246402\n",
      "Iteration 84, loss = 0.02993329\n",
      "Iteration 85, loss = 0.01055846\n",
      "Iteration 86, loss = 0.01027584\n",
      "Iteration 87, loss = 0.01094254\n",
      "Iteration 88, loss = 0.00988806\n",
      "Iteration 89, loss = 0.01161955\n",
      "Iteration 90, loss = 0.01052196\n",
      "Iteration 91, loss = 0.01104565\n",
      "Iteration 92, loss = 0.01059746\n",
      "Iteration 93, loss = 0.01125321\n",
      "Iteration 94, loss = 0.01120117\n",
      "Iteration 95, loss = 0.01322706\n",
      "Iteration 96, loss = 0.01062074\n",
      "Iteration 97, loss = 0.00947486\n",
      "Iteration 98, loss = 0.00988814\n",
      "Iteration 99, loss = 0.01647220\n",
      "Iteration 100, loss = 0.01106361\n",
      "Iteration 101, loss = 0.01140965\n",
      "Iteration 102, loss = 0.01214959\n",
      "Iteration 103, loss = 0.00888108\n",
      "Iteration 104, loss = 0.00881171\n",
      "Iteration 105, loss = 0.01009654\n",
      "Iteration 106, loss = 0.00998920\n",
      "Iteration 107, loss = 0.01048154\n",
      "Iteration 108, loss = 0.00977419\n",
      "Iteration 109, loss = 0.00887449\n",
      "Iteration 110, loss = 0.01240393\n",
      "Iteration 111, loss = 0.01236069\n",
      "Iteration 112, loss = 0.01072246\n",
      "Iteration 113, loss = 0.00912593\n",
      "Iteration 114, loss = 0.00955052\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.971830985915493\n"
     ]
    }
   ],
   "source": [
    "# vanilla classification\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_embeddings, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fft classification with sliding windows\n",
    "\n",
    "from split_utils import split_text\n",
    "\n",
    "all_docs_paras = [split_text(doc, segment_length=40) for doc in all_docs]\n",
    "\n",
    "# remove any empty paragraphs\n",
    "all_docs_paras = [[para for para in paras if para] for paras in all_docs_paras]\n",
    "# remove any '' paragraphs\n",
    "all_docs_paras = [[para for para in paras if para != ''] for paras in all_docs_paras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [02:51<00:00,  6.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for each paragraph\n",
    "print('Getting embeddings...')\n",
    "embeddings_engine = \"text-embedding-ada-002\"\n",
    "train_embeddings_paras = [get_embeddings(paras, engine=embeddings_engine) for paras in tqdm(all_docs_paras)]\n",
    "\n",
    "# pickle embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'wb') as f:\n",
    "    pickle.dump(train_embeddings_paras, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1063/1063 [00:07<00:00, 135.06it/s]\n",
      "100%|██████████| 1063/1063 [00:04<00:00, 234.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# load embeddings\n",
    "with open('embeddings/all_embeddings_paras.pkl', 'rb') as f:\n",
    "    all_embeddings_paras = pickle.load(f)\n",
    "\n",
    "# convert to numpy arrays\n",
    "all_embeddings_paras = [np.array(doc) for doc in all_embeddings_paras]\n",
    "\n",
    "# get FFTs\n",
    "def get_fft(embedding):\n",
    "    return np.abs(librosa.stft(embedding, n_fft=32, win_length=4))\n",
    "\n",
    "# lowpass filter\n",
    "def lowpass_filter(fft, cutoff=0.5):\n",
    "    \"\"\"\n",
    "    Lowpass filter for FFTs\n",
    "    \"\"\"\n",
    "    fft = fft.copy()\n",
    "    fft[:, int(cutoff*fft.shape[1]):] = 0\n",
    "    return fft\n",
    "\n",
    "# convert back to embeddings\n",
    "def fft_to_embedding(fft):\n",
    "    return librosa.istft(fft, win_length=4)\n",
    "\n",
    "# get FFTs\n",
    "all_embeddings_paras_fft = [get_fft(embedding) for embedding in tqdm(all_embeddings_paras)]\n",
    "\n",
    "# lowpass filter\n",
    "all_embeddings_paras_fft = [lowpass_filter(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "# convert back to embeddings\n",
    "all_embeddings_paras_lowpass = [fft_to_embedding(fft) for fft in tqdm(all_embeddings_paras_fft)]\n",
    "\n",
    "# average embeddings\n",
    "train_embeddings_lowpass_avg = [np.mean(embeddings, axis=0) for embeddings in all_embeddings_paras_lowpass]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70344484\n",
      "Iteration 2, loss = 0.68872113\n",
      "Iteration 3, loss = 0.69146077\n",
      "Iteration 4, loss = 0.68956520\n",
      "Iteration 5, loss = 0.68927693\n",
      "Iteration 6, loss = 0.68948456\n",
      "Iteration 7, loss = 0.68924747\n",
      "Iteration 8, loss = 0.68902725\n",
      "Iteration 9, loss = 0.68895248\n",
      "Iteration 10, loss = 0.68942516\n",
      "Iteration 11, loss = 0.68888414\n",
      "Iteration 12, loss = 0.68889813\n",
      "Iteration 13, loss = 0.68955780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "0.5305164319248826\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_embeddings_lowpass_avg, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# train classifier\n",
    "clf2 = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "# predict on test set\n",
    "y_pred = clf2.predict(X_test)\n",
    "\n",
    "# evaluate\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e67b88d6ad197c9402ec873fcbcf9f15e38aeaf8485f1023dc62e85e716efbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
